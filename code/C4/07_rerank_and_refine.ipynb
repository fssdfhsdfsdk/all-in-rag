{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_deepseek import ChatDeepSeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/myrag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入ColBERT重排器需要的模块\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_core.documents import Document\n",
    "from typing import Sequence\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColBERT重排器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ColBERTReranker(BaseDocumentCompressor):\n",
    "    \"\"\"ColBERT重排器\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        model_name = \"bert-base-uncased\"\n",
    "\n",
    "        # 加载模型和分词器\n",
    "        object.__setattr__(self, 'tokenizer', AutoTokenizer.from_pretrained(model_name))\n",
    "        object.__setattr__(self, 'model', AutoModel.from_pretrained(model_name))\n",
    "        self.model.eval()\n",
    "        print(f\"ColBERT模型加载完成\")\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        \"\"\"ColBERT文本编码\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def calculate_colbert_similarity(self, query_emb, doc_embs, query_mask, doc_masks):\n",
    "        \"\"\"ColBERT相似度计算（MaxSim操作）\"\"\"\n",
    "        scores = []\n",
    "\n",
    "        for i, doc_emb in enumerate(doc_embs):\n",
    "            doc_mask = doc_masks[i:i+1]\n",
    "\n",
    "            # 计算相似度矩阵\n",
    "            similarity_matrix = torch.matmul(query_emb, doc_emb.unsqueeze(0).transpose(-2, -1))\n",
    "\n",
    "            # 应用文档mask\n",
    "            doc_mask_expanded = doc_mask.unsqueeze(1)\n",
    "            similarity_matrix = similarity_matrix.masked_fill(~doc_mask_expanded.bool(), -1e9)\n",
    "\n",
    "            # MaxSim操作\n",
    "            max_sim_per_query_token = similarity_matrix.max(dim=-1)[0]\n",
    "\n",
    "            # 应用查询mask\n",
    "            query_mask_expanded = query_mask.unsqueeze(0)\n",
    "            max_sim_per_query_token = max_sim_per_query_token.masked_fill(~query_mask_expanded.bool(), 0)\n",
    "\n",
    "            # 求和得到最终分数\n",
    "            colbert_score = max_sim_per_query_token.sum(dim=-1).item()\n",
    "            scores.append(colbert_score)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        callbacks=None,\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"对文档进行ColBERT重排序\"\"\"\n",
    "        if len(documents) == 0:\n",
    "            return documents\n",
    "\n",
    "        # 编码查询\n",
    "        query_inputs = self.tokenizer(\n",
    "            [query],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            query_outputs = self.model(**query_inputs)\n",
    "            query_embeddings = F.normalize(query_outputs.last_hidden_state, p=2, dim=-1)\n",
    "\n",
    "        # 编码文档\n",
    "        doc_texts = [doc.page_content for doc in documents]\n",
    "        doc_inputs = self.tokenizer(\n",
    "            doc_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            doc_outputs = self.model(**doc_inputs)\n",
    "            doc_embeddings = F.normalize(doc_outputs.last_hidden_state, p=2, dim=-1)\n",
    "\n",
    "        # 计算ColBERT相似度\n",
    "        scores = self.calculate_colbert_similarity(\n",
    "            query_embeddings,\n",
    "            doc_embeddings,\n",
    "            query_inputs['attention_mask'],\n",
    "            doc_inputs['attention_mask']\n",
    "        )\n",
    "\n",
    "        # 排序并返回前5个\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        reranked_docs = [doc for doc, _ in scored_docs[:5]]\n",
    "\n",
    "        return reranked_docs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60168/604755049.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\")\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\", \n",
    "    temperature=0.1, \n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载和处理文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# 如果在 Notebook 中运行，__file__ 不存在，改用当前工作目录\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()  # 使用当前工作目录\n",
    "\n",
    "loader = TextLoader(SCRIPT_DIR / \"../../data/C4/txt/ai.txt\", encoding = \"utf-8\")\n",
    "docs = loader.load()\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunk_docs = text_spliter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建向量存储和基础检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stores = FAISS.from_documents(chunk_docs, embedding_model)\n",
    "base_retriever = vector_stores.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置ColBERT重排序器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColBERT模型加载完成\n"
     ]
    }
   ],
   "source": [
    "reranker = ColBERTReranker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置LLM压缩器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组装压缩管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[reranker, compressor]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建最终的压缩检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行查询并展示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 开始执行查询 ====================\n",
      "查询: AI还有哪些缺陷需要克服？\n",
      "\n",
      "--- (1) 基础检索结果 (Top 20) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60168/232063080.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  base_results = base_retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1] 行业巨头谷歌公司也没闲着。该公司在5月推出整体性能和智能推理能力均较以往版本大幅提升的多个“双子座2.5”系列模型，并发布了多个多模态模型，如图像生成模型Imagen 4和视频生成模型Veo 3，具备...\n",
      "\n",
      "  [2] 一个比较明显的问题是，AI生成内容虽然已非常流畅，但提供的信息很多时候还是不准确。5月，日本研究人员在德国《先进科学》杂志发表的一项研究成果中指出，这一问题与人类的语言障碍——失语症类似。\n",
      "\n",
      "    ...\n",
      "\n",
      "  [3] 业界也确实在努力从不同角度去寻求优化大模型的解决方案。中国科学院自动化研究所联合鹏城实验室提出了一种高效推理策略AutoThink，可让大模型实现自主切换思考模式，避免“过度思考”。\n",
      "\n",
      "    据研究...\n",
      "\n",
      "  [4] 一些国家已在积极尝试通过优化政策、法规来营造更好的AI创新环境。日本参议院全体会议5月28日以多数赞成票通过该国首部专门针对AI的法律，旨在促进AI相关技术研发和应用并防止其滥用。依据这部《人工智能相...\n",
      "\n",
      "  [5] 5月，全球多家科技公司发布新的大模型，它们在语义理解、多模态等方面进一步提升，人工智能（AI）的能力边界在不断扩大。随着无人驾驶、机器人等技术借助AI快速进化并逐步投入市场，不少国家通过推进法规建设、...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"AI还有哪些缺陷需要克服？\"\n",
    "print(f\"\\n{'='*20} 开始执行查询 {'='*20}\")\n",
    "print(f\"查询: {query}\\n\")\n",
    "\n",
    "# 7.1 基础检索结果\n",
    "print(f\"--- (1) 基础检索结果 (Top 20) ---\")\n",
    "base_results = base_retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(base_results):\n",
    "    print(f\"  [{i+1}] {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- (2) 管道压缩后结果 (ColBERT重排 + LLM压缩) ---\n",
      "  [1] 一个比较明显的问题是，AI生成内容虽然已非常流畅，但提供的信息很多时候还是不准确。\n",
      "\n",
      "  [2] 中国科学院自动化研究所联合鹏城实验室提出了一种高效推理策略AutoThink，可让大模型实现自主切换思考模式，避免“过度思考”。\n",
      "\n",
      "  [3] AI仍有不少缺陷需克服\n",
      "\n",
      "    尽管当前AI应用已相当广泛，但不少缺陷还是会影响其实用性。研究人员正努力分析导致这些缺陷的原因并寻求新的解决方法，从而改善AI的性能。\n",
      "\n",
      "    一个比较明显的问题是，AI生成内容虽然已非常流畅，但提供的信息很多时候还是不准确。5月，日本研究人员在德国《先进科学》杂志发表的一项研究成果中指出，这一问题与人类的语言障碍——失语症类似。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.2 使用管道压缩器的最终结果\n",
    "print(f\"\\n--- (2) 管道压缩后结果 (ColBERT重排 + LLM压缩) ---\")\n",
    "final_results = final_retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(final_results):\n",
    "    print(f\"  [{i+1}] {doc.page_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
